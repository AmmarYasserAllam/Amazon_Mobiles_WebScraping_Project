{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53faadb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page ended\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "\n",
    "# Initialize empty lists to store scraped data\n",
    "mobile_name = []\n",
    "bought_LM = []\n",
    "Mob_review = []\n",
    "feedback_score = []\n",
    "stock = []\n",
    "price = []\n",
    "prime_delivery_date = []\n",
    "link = []\n",
    "\n",
    "# Start scraping from page 1\n",
    "page_num = 1\n",
    "\n",
    "# Create an infinite loop to scrape multiple pages\n",
    "while True:\n",
    "    # Send a GET request to the Amazon page\n",
    "    result = requests.get(f\"https://www.amazon.eg/s?rh=n%3A21832883031&fs=true&language=en&ref=ref=sr_pg_{page_num}\")\n",
    "    src = result.content\n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "    # Check if we've reached the end of pages\n",
    "    if page_num > 80:\n",
    "        print('page ended')\n",
    "        break\n",
    "\n",
    "    # Extract data from the current page\n",
    "    mobile_names = soup.find_all('a', {'class': 'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal'})\n",
    "    boughts_LM = soup.find_all('span', {'class': 'a-size-base a-color-secondary'})\n",
    "    Mob_reviews = soup.find_all('span', {'class': 'a-size-base s-underline-text'})\n",
    "    feedback_scores = soup.find_all('i', {'class': 'a-icon a-icon-star-small a-star-small-3-5 aok-align-bottom'})\n",
    "    prices = soup.find_all('span', {'class': 'a-price-whole'})\n",
    "    prime_delivery_dates = soup.find_all('span', {'class': \"a-color-base a-text-bold\"})\n",
    "    stocks = soup.find_all('span', {'class': 'a-size-base a-color-price'})\n",
    "\n",
    "    # Ensure that the lengths of all lists are the same\n",
    "    # len(mobile_names) == len(Mob_reviews)\n",
    "\n",
    "    # Loop through the extracted data\n",
    "    for i in range(len(mobile_names)):\n",
    "        mobile_name.append(mobile_names[i].text)\n",
    "        link_complete = ('https://www.amazon.eg/' + mobile_names[i].attrs['href'])\n",
    "        link.append(link_complete)\n",
    "        bought_LM.append(boughts_LM[i].text)\n",
    "\n",
    "        # Check if there are enough 'Mob_review', otherwise assign a default value\n",
    "        if i < len(Mob_reviews):\n",
    "            Mob_review.append(Mob_reviews[i].text)\n",
    "        else:\n",
    "            # Assign \"N/A\" or any suitable default value\n",
    "            Mob_review.append(\"N/A\")\n",
    "\n",
    "        # Check if there are enough 'feedback_scores', otherwise assign a default value\n",
    "        if i < len(feedback_scores):\n",
    "            feedback_score.append(feedback_scores[i].text)\n",
    "        else:\n",
    "            # Assign \"N/A\" or any suitable default value\n",
    "            feedback_score.append(\"N/A\")\n",
    "\n",
    "        # Check if there are enough 'stocks', otherwise assign a default value\n",
    "        if i < len(stocks):\n",
    "            stock.append(stocks[i].text)\n",
    "        else:\n",
    "            # Assign \"N/A\" or any suitable default value\n",
    "            stock.append(\"N/A\")\n",
    "\n",
    "        # Check if there are enough 'prices', otherwise assign a default value\n",
    "        if i < len(prices):\n",
    "            price.append(prices[i].text)\n",
    "        else:\n",
    "            # Assign \"N/A\" or any suitable default value\n",
    "            price.append(\"N/A\")\n",
    "\n",
    "        # Check if there are enough 'prime delivery dates', otherwise assign a default value\n",
    "        if i < len(prime_delivery_dates):\n",
    "            prime_delivery_date.append(prime_delivery_dates[i].text)\n",
    "        else:\n",
    "            # Assign \"N/A\" or any suitable default value\n",
    "            prime_delivery_date.append(\"N/A\")\n",
    "\n",
    "    page_num += 1\n",
    "    print('page switched')\n",
    "\n",
    "# Create a CSV file and fill it with the scraped values\n",
    "filelist = [mobile_name, bought_LM, Mob_review, feedback_score, stock, price, prime_delivery_date, link]\n",
    "exported = zip_longest(*filelist, fillvalue='')\n",
    "\n",
    "with open(r'G:\\Courses\\Data science\\Role Models\\Codezilla\\Web Scraping/MobAmazonScraping.csv', 'w', newline='', encoding='utf-8') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow(['mobile name', 'Bought Last Month', 'Mobile review', 'Feedback score', 'Stock', 'Price', 'Prime Delivery Date', 'link'])\n",
    "    wr.writerows(exported)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f873f5f",
   "metadata": {},
   "source": [
    "#### Author :\n",
    "### Ammar Allam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
